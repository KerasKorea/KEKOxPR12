{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR-027 Glove\n",
    "\n",
    "곽근봉 님의 [Glove 강의](https://www.youtube.com/watch?v=uZ2GtEe-50E&list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS&index=28) 감사드립니다.\n",
    "\n",
    "word embedding이 처음이시라면 word2vector.ipynb 부터 보시는 걸 추천드립니다.\n",
    "\n",
    "Glove를 이용해 간단한 문장을 학습시킵니다.\n",
    "\n",
    "[tensorflow-glove](https://github.com/GradySimon/tensorflow-glove) 코드를 simple하게 옮겨보았습니다.\n",
    "\n",
    "논문: https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove 간단히 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 문장 하나를 학습시켜 보면서 glove를 간단히 살펴보겠습니다.\n",
    "\n",
    "<code>내가 그의 이름을 불러주었을 때, 그는 내게로 와 꽃이 되었다.</code>\n",
    "\n",
    "데이터는 word2vec 처럼 가공을 합니다.\n",
    "\n",
    "window_size 가 1일 때의 예입니다.\n",
    "\n",
    "(word, context)의 형태\n",
    "```\n",
    "(그의, 내가)\n",
    "(그의, 이름을)\n",
    "(이름을, 그의)\n",
    "(이름을, 불러주었을)\n",
    "(내게로, 그는)\n",
    "(내게로, 와)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그의', '불러주었을', '내게로', '그는', '때,', '이름을', '되었다.', '와', '꽃이', '내가'] \n",
      "\n",
      "{'그의': 0, '불러주었을': 1, '내게로': 2, '때,': 4, '그는': 3, '이름을': 5, '내가': 9, '와': 7, '되었다.': 6, '꽃이': 8} \n",
      "\n",
      "{(7, 3): 0.5, (4, 7): 0.3333333333333333, (1, 3): 0.5, (9, 1): 0.3333333333333333, (5, 9): 1.0, (2, 8): 0.5, (3, 2): 1.0, (2, 1): 1.0, (6, 2): 1.0, (3, 7): 0.5, (5, 1): 1.0, (7, 2): 0.3333333333333333, (4, 0): 1.0, (1, 2): 0.3333333333333333, (7, 4): 1.0, (9, 0): 1.0, (6, 7): 0.5, (7, 6): 0.5, (1, 5): 0.3333333333333333, (5, 0): 0.5, (0, 4): 0.3333333333333333, (8, 6): 1.0, (3, 5): 1.0, (4, 1): 0.3333333333333333, (0, 1): 0.5, (5, 4): 0.5, (2, 6): 0.3333333333333333, (8, 2): 0.5, (4, 5): 0.5, (1, 4): 1.0, (2, 3): 0.3333333333333333, (1, 9): 1.0, (8, 7): 0.3333333333333333, (9, 5): 0.5, (4, 2): 0.5, (1, 0): 0.5, (5, 3): 0.3333333333333333, (2, 7): 1.0, (8, 3): 1.0, (6, 8): 0.3333333333333333, (3, 4): 0.3333333333333333, (3, 1): 0.5, (3, 8): 0.3333333333333333, (4, 3): 1.0, (0, 9): 1.0, (0, 5): 1.0, (7, 8): 1.0, (2, 4): 0.5} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 현재 케라스에는 glove에서 필요한 cooccurrence matrix를 구하며 깔끔하게 sampling 해주는 함수가 없습니다. (ㅠㅅㅠ)\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "txt = \"내가 그의 이름을 불러주었을 때, 그는 내게로 와 꽃이 되었다.\"\n",
    "datas = txt.split();\n",
    "\n",
    "# hyperparameter\n",
    "vocab_size = 1000    # 총 단어 개수\n",
    "embed_size = 100     # 임베딩 할 사이즈\n",
    "ws = 3               # window size\n",
    "min_occurrences = 0  # 제외할 단어 빈도수\n",
    "\n",
    "# 각 단어의 빈도수 체크\n",
    "word_counts = Counter()\n",
    "word_counts.update(datas)\n",
    "\n",
    "#### co occurrence matrix 및 sample 구하기\n",
    "co_occurrence_count = defaultdict(float)\n",
    "data_length = len(datas)\n",
    "\n",
    "for i, word in enumerate(datas):\n",
    "    # 완전 탐색을 하며 left context, word, right context를 얻어냅니다.\n",
    "    if i < ws:\n",
    "        l_context = datas[:i]\n",
    "        r_context = datas[i+1:i+ws+1]\n",
    "    elif i == data_length - 1:\n",
    "        l_context = datas[i-ws:i]\n",
    "        r_context = []\n",
    "    elif i >= data_length - ws:\n",
    "        l_context = datas[i-ws:i]\n",
    "        r_context = datas[i+1:]\n",
    "    else:\n",
    "        l_context = datas[i-ws:i]\n",
    "        r_context = datas[i+1:i+ws+1]\n",
    "    \n",
    "    # co occurrence matrix 를 구합니다.\n",
    "    for i, context_word in enumerate(l_context):\n",
    "        co_occurrence_count[(word, context_word)] += 1 / (i + 1)\n",
    "    for i, context_word in enumerate(r_context):\n",
    "        co_occurrence_count[(word, context_word)] += 1 / (i + 1)\n",
    "        \n",
    "### 구한 count를 토대로 words 사전과 cooccurrence matrix를 만듭니다.\n",
    "# min occurrences가 넘는 words 만 사용합니다.\n",
    "words = [word for word, count in word_counts.most_common(vocab_size) if count >= min_occurrences]\n",
    "word_to_id = {word: i for i, word in enumerate(words)}\n",
    "co_occurrence_matrix = {\n",
    "    (word_to_id[words[0]], word_to_id[words[1]]): count\n",
    "    for words, count in co_occurrence_count.items()\n",
    "    if words[0] in word_to_id and words[1] in word_to_id}\n",
    "\n",
    "print(words, '\\n')\n",
    "print(word_to_id, '\\n')\n",
    "print(co_occurrence_matrix, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 짜 봅시다!\n",
    "\n",
    "케라스의 [**Embedding layer**](https://keras.io/layers/embeddings/)는 int형 argument들을 vector화 시키는 layer 입니다.\n",
    "\n",
    "예를 들어\n",
    "```\n",
    "[[4], [20]]\n",
    "```\n",
    "이 입력되었다면 다음과 같이 output을 반환시켜 줄 수 있습니다.\n",
    "```\n",
    "[[0.25, 0.1], [0.6, -0.2]]\n",
    "```\n",
    "\n",
    "4 라는 수가 \\[0.25, 0.1\\] 이라는 벡터로 변한 것이죠.\n",
    "\n",
    "공식대로 \n",
    "``` python\n",
    "word_model_out * context_model_out + b1 + b2 == log(co_occurrence_count)\n",
    "```\n",
    "가 되도록 만들어 줍시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 100)          100000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 100)          100000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 1)            1000        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            1000        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dot_1[0][0]                      \n",
      "                                                                 sequential_2[1][0]               \n",
      "                                                                 sequential_4[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 202,000\n",
      "Trainable params: 202,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import Dot, Add\n",
    "from keras.layers.core import Dense, Reshape, Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "import keras.backend as K\n",
    "\n",
    "# word model\n",
    "model_w = Sequential()\n",
    "model_w.add(Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "model_w.add(Flatten())\n",
    "\n",
    "# word bias model\n",
    "model_wb = Sequential()\n",
    "model_wb.add(Embedding(vocab_size, 1, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "model_wb.add(Flatten())\n",
    "    \n",
    "# context model\n",
    "model_c = Sequential()\n",
    "model_c.add(Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "model_c.add(Flatten())\n",
    "\n",
    "# context bias model\n",
    "model_cb = Sequential()\n",
    "model_cb.add(Embedding(vocab_size, 1, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "model_cb.add(Flatten())\n",
    "\n",
    "# 전체 모델 작성\n",
    "input_w = Input(shape=(None,))\n",
    "input_c = Input(shape=(None,))\n",
    "\n",
    "output_w = model_w(input_w)\n",
    "output_wb = model_wb(input_w)\n",
    "output_c = model_c(input_c)\n",
    "output_cb = model_cb(input_c)\n",
    "\n",
    "# word model과 context model의 output들을 내적시킵니다.\n",
    "x = Dot(axes=1)([output_w, output_c])\n",
    "\n",
    "# bias들을 더합니다.\n",
    "x = Add()([x, output_wb, output_cb])\n",
    "\n",
    "model = Model([input_w, input_c], x)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "X1 = []\n",
    "X2 = []\n",
    "Y = []\n",
    "for i in co_occurrence_matrix.items():\n",
    "    X1.append(i[0][0])\n",
    "    X2.append(i[0][1])\n",
    "    Y.append(math.log(i[1]))\n",
    "    \n",
    "X1 = np.array(X1)\n",
    "X2 = np.array(X2)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.5086\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 0s 245us/step - loss: 0.4971\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 0s 187us/step - loss: 0.4867\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 0s 226us/step - loss: 0.4771\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 0s 276us/step - loss: 0.4674\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 0s 227us/step - loss: 0.4582\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 0s 343us/step - loss: 0.4486\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 0s 307us/step - loss: 0.4395\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 0s 265us/step - loss: 0.4306\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 0s 242us/step - loss: 0.4209\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 0s 484us/step - loss: 0.4120\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 0s 239us/step - loss: 0.4030\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 0s 209us/step - loss: 0.3937\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 0s 241us/step - loss: 0.3846\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 0s 324us/step - loss: 0.3752\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 0s 381us/step - loss: 0.3666\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 0s 306us/step - loss: 0.3565\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 0s 272us/step - loss: 0.3473\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 0s 345us/step - loss: 0.3380\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 0s 281us/step - loss: 0.3284\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 0s 270us/step - loss: 0.3190\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 0s 406us/step - loss: 0.3085\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 0s 351us/step - loss: 0.2996\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 0s 215us/step - loss: 0.2889\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 0s 272us/step - loss: 0.2796\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 0s 242us/step - loss: 0.2700\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 0s 338us/step - loss: 0.2595\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 0s 323us/step - loss: 0.2498\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 0s 340us/step - loss: 0.2396\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 0s 302us/step - loss: 0.2299\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 0s 316us/step - loss: 0.2201\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 0s 345us/step - loss: 0.2109\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 0s 289us/step - loss: 0.2007\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 0s 204us/step - loss: 0.1916\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 0s 218us/step - loss: 0.1818\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 0s 198us/step - loss: 0.1725\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 0s 206us/step - loss: 0.1630\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 0s 226us/step - loss: 0.1539\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 0s 147us/step - loss: 0.1450\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 0s 187us/step - loss: 0.1365\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 0s 306us/step - loss: 0.1275\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 0s 358us/step - loss: 0.1194\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 0s 319us/step - loss: 0.1115\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 0s 250us/step - loss: 0.1030\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 0s 427us/step - loss: 0.0957\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 0s 352us/step - loss: 0.0882\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 0s 254us/step - loss: 0.0813\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 0s 291us/step - loss: 0.0748\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 0s 275us/step - loss: 0.0681\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 0s 187us/step - loss: 0.0622\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 0s 476us/step - loss: 0.0567\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 0s 345us/step - loss: 0.0508\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 0s 289us/step - loss: 0.0460\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 0s 402us/step - loss: 0.0415\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 0s 300us/step - loss: 0.0368\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 0s 367us/step - loss: 0.0328\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 0s 314us/step - loss: 0.0292\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 0s 292us/step - loss: 0.0257\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 0s 371us/step - loss: 0.0225\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 0s 262us/step - loss: 0.0198\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 0s 287us/step - loss: 0.0173\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 0s 266us/step - loss: 0.0151\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 0s 317us/step - loss: 0.0131\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 0s 345us/step - loss: 0.0112\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 0s 337us/step - loss: 0.0096\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 0s 205us/step - loss: 0.0083\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 0s 221us/step - loss: 0.0070\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 0s 235us/step - loss: 0.0061\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 0s 220us/step - loss: 0.0051\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 0s 226us/step - loss: 0.0043\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 0s 368us/step - loss: 0.0037\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 0s 323us/step - loss: 0.0030\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 0s 212us/step - loss: 0.0026\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 0s 322us/step - loss: 0.0021\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 0s 275us/step - loss: 0.0018\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 0.0015\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 0s 223us/step - loss: 0.0013\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 0s 176us/step - loss: 0.0011\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 0s 311us/step - loss: 8.9932e-04\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 0s 245us/step - loss: 7.7393e-04\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 6.6554e-04\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 0s 361us/step - loss: 5.7433e-04\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 0s 246us/step - loss: 4.9386e-04\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 0s 220us/step - loss: 4.2887e-04\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 0s 338us/step - loss: 3.8165e-04\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 0s 234us/step - loss: 3.3166e-04\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 0s 291us/step - loss: 3.0084e-04\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 0s 282us/step - loss: 2.7065e-04\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 0s 203us/step - loss: 2.3849e-04\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 0s 328us/step - loss: 2.2059e-04\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 0s 209us/step - loss: 1.9907e-04\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 0s 195us/step - loss: 1.7959e-04\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 0s 431us/step - loss: 1.6548e-04\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 0s 321us/step - loss: 1.5076e-04\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 0s 275us/step - loss: 1.3860e-04\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 0s 343us/step - loss: 1.2703e-04\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 0s 265us/step - loss: 1.1716e-04\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 270us/step - loss: 1.0728e-04\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 0s 213us/step - loss: 9.8929e-05\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 0s 359us/step - loss: 9.1019e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0a82c7c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습시킵니다.\n",
    "model.fit([X1, X2], Y,\n",
    "          epochs=100, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 사용하는 부분은 model_w의 embedding layer 입니다.\n",
    "\n",
    "임베딩하는 방식이 중요한 것이지요.\n",
    "\n",
    "model_w 에 벡터화 시키고 싶은 수를 넣으면 결과가 나올 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04126682  0.02041662  0.07496435 -0.07006773  0.06896963  0.05239534\n",
      "  -0.06739062 -0.02732384  0.07687686 -0.10646415  0.08526754 -0.11001399\n",
      "   0.16387607  0.11731478 -0.09811315 -0.16818208 -0.10618692  0.10006183\n",
      "  -0.07625895 -0.0956468  -0.16605097 -0.0733607   0.05235191  0.05943675\n",
      "  -0.05344569 -0.16186695 -0.06868561 -0.07074318  0.13461851  0.07791412\n",
      "  -0.02670521  0.17489552  0.10048576  0.05107064 -0.15255289 -0.07233072\n",
      "  -0.04385498 -0.10624173 -0.07228147  0.10215108 -0.12747085 -0.01945782\n",
      "   0.15561906 -0.18081762 -0.00810576 -0.02820941  0.06056061  0.15553233\n",
      "   0.10657778 -0.14834158 -0.11995964 -0.02345999  0.16221862  0.00799892\n",
      "   0.10882306  0.13122565 -0.11651555 -0.12902538  0.04407527 -0.01213415\n",
      "   0.02516619 -0.16075747  0.18391143  0.01563227  0.05561586  0.14834054\n",
      "   0.15524881 -0.16697121 -0.14766945  0.02491257  0.11908259 -0.0445068\n",
      "  -0.09575619 -0.11983696  0.09935905 -0.00665964  0.16705258  0.17732404\n",
      "   0.17635936  0.08531837  0.13246793  0.00554059 -0.1237716   0.00253504\n",
      "  -0.11886234 -0.15523161  0.06155805  0.09640175  0.1450057  -0.00657756\n",
      "   0.10650004 -0.10182657  0.11998048  0.03098256 -0.11404245  0.0136621\n",
      "  -0.1373973  -0.03138766  0.04506603 -0.00584891]]\n"
     ]
    }
   ],
   "source": [
    "print(model_w.predict(np.array([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 학습된 glove를 사용해 보고 싶으시다면 아래 링크를 참고하시면 좋겠습니다.\n",
    "\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드에서 문제가 되는 부분이 있다면 꼭 연락 주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact me\n",
    "케라스를 사랑하는 개발자 입니다.\n",
    "\n",
    "질문, 조언, contribtuion 등 소통은 언제나 환영합니다.\n",
    "\n",
    "Anthony Kim(김동현) : artit.anthony@gmail.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
