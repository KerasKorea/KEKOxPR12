{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec model에 대해서 잘 모르시는 분들을 위해 만들어 보았습니다.\n",
    "\n",
    "논문: https://arxiv.org/pdf/1301.3781.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec은 다음과 같은 2가지 모델이 있습니다.\n",
    "\n",
    "1. CBOW(Continuous Bag of Words)\n",
    "2. Skip-gram\n",
    "\n",
    "아직까지도 많이 사용되는 word2vec 은 인상 깊게도 아주 얕은 신경망입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "\n",
    "gensim 모듈은 쉽게 word embedding을 제공해주는 라이브러리 입니다.\n",
    "\n",
    "NLP 분야에서의 scikit-learn 정도로 생각하시면 되겠습니다.\n",
    "\n",
    "<code>pip install gensim</code>\n",
    "\n",
    "<code>pip install wget</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "사용할 데이터셋은 text8 데이터셋입니다.\n",
    "\n",
    "text8 데이터셋은 위키피디아 데이터셋인 enwik9 의 test 셋을 이용하여 만든 아주 작은 데이터셋 입니다. \n",
    "\n",
    "아주 작은 데이터셋이라 하지만 17,005,207 단어를 담고 있어서 간단한 테스트를 하기 좋은 데이터셋이죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT modules\n",
    "import wget\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text8 데이터셋을 다운받고\n",
    "wget.download('http://mattmahoney.net/dc/text8.zip')\n",
    "\n",
    "# 압축풀기를 진행합니다.\n",
    "text8_zip = zipfile.ZipFile('text8.zip')\n",
    "text8_zip.extractall()\n",
    "text8_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 간단한 예제를 통해 skig-gram과 CBOW를 보고 난 후 text8 데이터를 학습시키겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip gram 모델은 현재 단어를 가지고 주변의 단어를 예측합니다.\n",
    "\n",
    "예를 들어,\n",
    "\n",
    "<code>내가 그의 이름을 불러주었을 때, 그는 내게로 와 꽃이 되었다.</code>\n",
    "\n",
    "라는 문장이 있다면 이런 식으로 분해할 수가 있습니다.\n",
    "\n",
    "```\n",
    "(그의, [내가, 이름을])\n",
    "(이름을, [그의, 불러주었을])\n",
    "(내게로, [그는, 와])\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "위의 예시는 (단어, context(문맥)) 형태의 조합입니다.\n",
    "\n",
    "window_size 는 여기서 1 입니다. context는 좌우 단어 1개 씩 만 보는 것이죠.\n",
    "\n",
    "만약 window_size가 2라면 \n",
    "\n",
    "```\n",
    "(이름을, [내가, 그의, 불러주었을, 때]) \n",
    "...\n",
    "```\n",
    "식으로 결과가 나오게 됩니다.\n",
    "\n",
    ".\n",
    "\n",
    "스킵 그램 모델은 중간 단어가 입력이 되어지면 주변의 단어를 출력하는 모델입니다.\n",
    "\n",
    "학습을 위해서 분해된 데이터를 다음과 같이 변환시킬 수 있겠죠.\n",
    "\n",
    "\n",
    "```\n",
    "(그의, 내가)\n",
    "(그의, 이름을)\n",
    "(이름을, 그의)\n",
    "(이름을, 불러주었을)\n",
    "(내게로, 그는)\n",
    "(내게로, 와)\n",
    "...\n",
    "```\n",
    "\n",
    "이 모델을 학습시키기 위해서는 negative 데이터도 필요합니다.\n",
    "\n",
    "아래와 같이 전혀 다른 조합들을 만들어 내서 1:1 만큼의 데이터를 만들어 내고 학습을 시켜야 합니다.\n",
    "\n",
    "```\n",
    "(그의, 꽃이)\n",
    "(이름을, 내게로)\n",
    "(내게로, 불러주었을)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스의 [**Embedding layer**](https://keras.io/layers/embeddings/)는 int형 argument들을 vector화 시키는 layer 입니다.\n",
    "\n",
    "예를 들어\n",
    "```\n",
    "[[4], [20]]\n",
    "```\n",
    "이 입력되었다면 다음과 같이 output을 반환시켜 줄 수 있습니다.\n",
    "```\n",
    "[[0.25, 0.1], [0.6, -0.2]]\n",
    "```\n",
    "\n",
    "4 라는 수가 \\[0.25, 0.1\\] 이라는 벡터로 변한 것이죠.\n",
    "\n",
    "가장 단순히 위에서 정의했던 한글 문장을 학습시켜 볼까요?\n",
    "\n",
    "[**tokenizer**](https://keras.io/preprocessing/text/#tokenizer)를 이용하면 간편히 문장을 preprocessing 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2id: \n",
      " {'불러주었을': 4, '꽃이': 9, '내게로': 7, '이름을': 3, '되었다': 10, '그의': 2, '때': 5, '내가': 1, '와': 8, '그는': 6} \n",
      "\n",
      "id2word: \n",
      " {1: '내가', 2: '그의', 3: '이름을', 4: '불러주었을', 5: '때', 6: '그는', 7: '내게로', 8: '와', 9: '꽃이', 10: '되었다'} \n",
      "\n",
      "wids: \n",
      " [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
      "\n",
      "----------After skipgrams()----------\n",
      "\n",
      "pairs: \n",
      " [[2, 3], [3, 2], [7, 1], [2, 7], [8, 7], [6, 7], [2, 1], [7, 6], [8, 9], [8, 6], [7, 1], [10, 8], [1, 1], [9, 7], [1, 2], [6, 6], [9, 1], [5, 6], [2, 3], [4, 8], [3, 2], [8, 7], [5, 3], [3, 8], [4, 3], [5, 4], [6, 5], [3, 4], [4, 5], [10, 9], [9, 8], [4, 1], [6, 7], [9, 10], [5, 6], [7, 8]] \n",
      "\n",
      "labels: \n",
      " [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "txt = \"내가 그의 이름을 불러주었을 때, 그는 내게로 와 꽃이 되었다.\"\n",
    "\n",
    "# tokenizer는 고유한 단어를 정수ID로 매핑해 줍니다.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([txt])\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "print('word2id: \\n', word2id, '\\n')\n",
    "print('id2word: \\n', id2word, '\\n')\n",
    "\n",
    "# txt를 word 리스트로 바꾼 후 각각에 해당하는 단어들을 word2id dictionary 의 인덱스로 넣습니다.\n",
    "wids = [word2id[w] for w in text_to_word_sequence(txt)]\n",
    "print('wids: \\n', wids, '\\n')\n",
    "\n",
    "# 이 부분이 참 편합니다. skipgrams 함수는 자동으로 데이터를 sampling 해 줍니다.\n",
    "pairs, labels = skipgrams(wids, len(word2id), \n",
    "                          window_size=1, \n",
    "                          negative_samples=1., \n",
    "                          shuffle=True,\n",
    "                          categorical=False, \n",
    "                          sampling_table=None)\n",
    "\n",
    "print('----------After skipgrams()----------\\n')\n",
    "print('pairs: \\n', pairs, '\\n')\n",
    "print('labels: \\n', labels)\n",
    "\n",
    "# 학습을 위해 데이터를 numpy로 바꾸어 줍니다.\n",
    "pairs = np.array(pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "transposed = pairs.T\n",
    "X1 = transposed[0]\n",
    "X2 = transposed[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pairs는 보이는 대로 두 수의 조합이고 labels 는 sample이 positive라면 1, negative라면 0을 갖는 리스트 입니다.\n",
    "\n",
    "negative sample 을 뽑을 때 skipgrams 함수가 완전히 랜덤으로 뽑기 때문에 positive 한 pair가 label 0 이 달리기도 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 500)          5000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 500)          5000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 10,000,002\n",
      "Trainable params: 10,000,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import Dot\n",
    "from keras.layers.core import Dense, Reshape, Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "import keras.backend as K\n",
    "\n",
    "w_size = 10000     # 갖고 있는 단어의 종류를 10000개,\n",
    "embed_size = 500   # 출력 벡터의 크기를 500으로 둡니다.\n",
    "\n",
    "# word model\n",
    "model_w = Sequential()\n",
    "model_w.add(Embedding(w_size, embed_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "model_w.add(Flatten())\n",
    "    \n",
    "# context model\n",
    "model_c = Sequential()\n",
    "model_c.add(Embedding(w_size, embed_size, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "model_c.add(Flatten())\n",
    "\n",
    "# 전체 모델 작성\n",
    "# word model과 context model의 output들을 내적시키며 합칩니다.\n",
    "input_w = Input(shape=(None,))\n",
    "input_c = Input(shape=(None,))\n",
    "\n",
    "output_w = model_w(input_w)\n",
    "output_c = model_c(input_c)\n",
    "\n",
    "x = Dot(axes=1)([output_w, output_c])\n",
    "\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([input_w, input_c], x)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2499\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2479\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2464\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2450\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2435\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2421\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2405\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2390\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2373\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2357\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2338\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2319\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2300\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2277\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2255\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2231\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2204\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2178\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.2149\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2119\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2088\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2056\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2021\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1987\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1949\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1911\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1873\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1832\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1792\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1751\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.1709\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1668\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1626\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1586\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1541\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1501\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1462\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1424\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1384\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1350\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1313\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1279\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1247\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1218\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1186\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1160\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1133\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1109\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1086\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1065\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1044\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1026\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1008\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0990\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0975\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0958\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0947\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0931\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0916\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0905\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0894\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0883\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0874\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0863\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0857\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0850\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0846\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0835\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0829\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0821\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0817\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0812\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0807\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0803\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0797\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0792\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0790\n",
      "Epoch 78/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0789\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0787\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0786\n",
      "Epoch 81/100\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0790\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0787\n",
      "Epoch 83/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0787\n",
      "Epoch 84/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0789\n",
      "Epoch 85/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0785\n",
      "Epoch 86/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0785\n",
      "Epoch 87/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0784\n",
      "Epoch 88/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0779\n",
      "Epoch 89/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0780\n",
      "Epoch 90/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0781\n",
      "Epoch 91/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0782\n",
      "Epoch 92/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0782\n",
      "Epoch 93/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0782\n",
      "Epoch 94/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0780\n",
      "Epoch 95/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0777\n",
      "Epoch 96/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0776\n",
      "Epoch 97/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0774\n",
      "Epoch 98/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0774\n",
      "Epoch 99/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0772\n",
      "Epoch 100/100\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9165b2278>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습시킵니다.\n",
    "model.fit([X1, X2], labels,\n",
    "          epochs=100, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 사용하는 부분은 model_w의 embedding layer 입니다.\n",
    "\n",
    "임베딩하는 방식이 중요한 것이지요.\n",
    "\n",
    "model_w 에 벡터화 시키고 싶은 수를 넣으면 결과가 나올 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0743627  -0.03843859 -0.01977107 -0.04395692 -0.03147187 -0.07424004\n",
      "   0.01381021 -0.02287102 -0.08419351  0.02789069 -0.05809529 -0.09313765\n",
      "   0.03501471  0.06392162 -0.06250969  0.03329175  0.08057152 -0.05717527\n",
      "  -0.01263071  0.04328177 -0.0731423  -0.04852783  0.0962188  -0.1013721\n",
      "  -0.08040303  0.12037461  0.00769393 -0.02000787 -0.06813817 -0.08545297\n",
      "  -0.0185378   0.01688842 -0.02047679 -0.05962992 -0.04247573  0.04836977\n",
      "   0.1039657  -0.03758182  0.09900548  0.04903841 -0.01611881  0.09051777\n",
      "   0.0248059  -0.06109172 -0.08599535  0.06141053 -0.06520886 -0.12550041\n",
      "  -0.1029128  -0.00272258 -0.10655595 -0.00536699  0.03118335  0.01080526\n",
      "  -0.05436905  0.08979445  0.08450153 -0.06823603 -0.01001124  0.08720243\n",
      "  -0.05166619 -0.04097356  0.10202493 -0.02589416 -0.02685894 -0.00446152\n",
      "  -0.01319941  0.06244966  0.04766689 -0.06107156  0.09596038 -0.00432958\n",
      "  -0.02389773 -0.03806435 -0.10272231  0.08900584 -0.12174722 -0.05168158\n",
      "  -0.08427301 -0.05004458  0.11858637  0.09163648  0.04909391 -0.07177602\n",
      "  -0.08851818  0.04415789 -0.08679543  0.0306872  -0.03543807  0.03332926\n",
      "  -0.05573472 -0.02760573 -0.03267325 -0.04100187 -0.10138174  0.00493041\n",
      "   0.0428196  -0.12060203  0.07026189  0.10115605 -0.12240794  0.03441084\n",
      "  -0.08134835 -0.02874639 -0.03490637 -0.05538534 -0.07476979  0.06562679\n",
      "   0.10500608 -0.08911642  0.04494607 -0.101643    0.05869732 -0.06717874\n",
      "   0.0292535   0.11178794  0.03260467  0.08919243  0.08104097 -0.03140382\n",
      "  -0.04770828 -0.01704362 -0.07778049 -0.12374797 -0.11700308 -0.0102704\n",
      "   0.05345402  0.11863986  0.00545411  0.06797927  0.03416415  0.05066106\n",
      "   0.02831099 -0.07939095  0.10823129 -0.07737297 -0.11601665 -0.090269\n",
      "   0.0237294   0.01992787 -0.08630652  0.04949065 -0.05814885  0.11488923\n",
      "   0.01398542 -0.0042863  -0.01829264  0.02151614  0.08852348  0.05923502\n",
      "  -0.00978893  0.10768544  0.07281587 -0.08382625  0.03282678  0.04970123\n",
      "  -0.06454221  0.05842583 -0.03966223  0.04205285  0.01291698  0.0839551\n",
      "  -0.0628964   0.10067293  0.07019806  0.02793513 -0.03929118 -0.11610232\n",
      "   0.11652986 -0.07618054  0.05374066  0.01790612  0.01482373 -0.11225039\n",
      "  -0.08460525 -0.12365805 -0.11702573  0.0592754  -0.04550154  0.03906772\n",
      "  -0.0289805   0.04444942  0.03233723  0.09235065  0.10918204  0.02282459\n",
      "  -0.03336577  0.08403381  0.0460236   0.03152954  0.02632944 -0.07591225\n",
      "  -0.02099748 -0.06606623 -0.00994009  0.06093714  0.0332463   0.01441601\n",
      "   0.08350666  0.07254224 -0.05639594  0.06878488  0.10932588  0.10938451\n",
      "   0.01708572 -0.03667371 -0.04798282 -0.0501771   0.05383058  0.09175258\n",
      "   0.03630393 -0.06662018  0.11944225 -0.0142671   0.05363816 -0.05230125\n",
      "  -0.07894578  0.06664549 -0.12371157  0.12221903 -0.07627589  0.00055497\n",
      "   0.05992029 -0.00437169 -0.03632006 -0.03191921 -0.06131383 -0.08902213\n",
      "   0.0105677   0.06641898 -0.11587349  0.0001757   0.06081988 -0.01515756\n",
      "  -0.03872042 -0.07464305  0.05752107  0.09092024  0.01449615 -0.06297668\n",
      "  -0.06720219 -0.04729193 -0.07870357 -0.11730339  0.09141781  0.10392556\n",
      "   0.03219302 -0.07617418 -0.05860415  0.06332971  0.0354655  -0.11628783\n",
      "   0.02155284 -0.07720642  0.06448735  0.0525602   0.10012815 -0.07849681\n",
      "   0.05130362  0.07752115 -0.00215195 -0.02188509  0.08898904 -0.06998472\n",
      "   0.09709353 -0.10542946 -0.0866545  -0.08069264 -0.10932755 -0.00813514\n",
      "  -0.01136714  0.07690798  0.05151327  0.06187023 -0.10636115  0.04353967\n",
      "   0.06772848 -0.05639536  0.09663    -0.04421371 -0.02239288 -0.12865917\n",
      "   0.09613311 -0.05938504 -0.07941347  0.01924296  0.08413088  0.10404429\n",
      "   0.07157506  0.08145431 -0.08137684  0.08374186 -0.00309848 -0.1053279\n",
      "  -0.04664136  0.07406341  0.01406985  0.04254053  0.05824166  0.05753592\n",
      "  -0.08447839 -0.1260618  -0.02197405  0.05227724  0.02299747 -0.10002702\n",
      "   0.00694781 -0.04485056  0.098021   -0.06853224 -0.08772014  0.01574702\n",
      "  -0.02373869 -0.09929916  0.07773695  0.06843717 -0.04386298  0.01998943\n",
      "   0.03215303  0.10108341 -0.0622117  -0.01709699 -0.00861595  0.06230029\n",
      "   0.06153743  0.02147603  0.00846854 -0.02689828  0.10346051 -0.0667729\n",
      "  -0.08824553  0.0281343  -0.01947356 -0.04997022  0.05737672 -0.03624668\n",
      "   0.05285649 -0.02773351 -0.08649816 -0.09206133 -0.0719561   0.08818793\n",
      "  -0.03331636  0.08945893  0.11784647 -0.01824817  0.02203279  0.05855503\n",
      "  -0.10948368  0.09827708 -0.02054802  0.08465479 -0.06619807  0.01048742\n",
      "   0.01269027  0.09642616 -0.05195416  0.08293283  0.00824885  0.07504608\n",
      "   0.07058471 -0.04296447  0.09248137 -0.09451388 -0.04848403  0.00656324\n",
      "  -0.02557443  0.00740948  0.0852421   0.03500204 -0.02757328 -0.06907686\n",
      "  -0.07073158 -0.05145128  0.12449013  0.07222442 -0.07295077  0.02684337\n",
      "  -0.08933716 -0.07508706 -0.10692944  0.07967166 -0.10316446  0.00496784\n",
      "   0.03738666 -0.12623718  0.05814891 -0.04612418  0.03354667 -0.11561084\n",
      "  -0.00345949  0.03799894  0.06623574 -0.0338924  -0.00228482 -0.07513537\n",
      "   0.10387697 -0.08606716 -0.05997741  0.01164641  0.12608129 -0.05511576\n",
      "  -0.06391685  0.0202737  -0.07174893  0.0808818   0.10619222  0.11927526\n",
      "  -0.05588565  0.05143211 -0.0890038  -0.07762695 -0.0869458   0.03951811\n",
      "  -0.01732192 -0.10553373 -0.05072371  0.06264659  0.08273474 -0.0186051\n",
      "   0.02429676  0.0832716   0.06396925  0.08650361 -0.00852304 -0.08098662\n",
      "   0.09422665 -0.00649139 -0.00890251  0.0052421   0.0469445   0.07688385\n",
      "   0.10363901  0.04242203  0.07583559  0.01220717 -0.05269868  0.08840245\n",
      "   0.03342078  0.06218325 -0.01997889  0.08157205 -0.08403002  0.04942667\n",
      "   0.08657887  0.1388143   0.03584305 -0.00638059 -0.06535269  0.06315264\n",
      "   0.11296492  0.01318976  0.06438261  0.08522508  0.09959076 -0.0714679\n",
      "   0.10370631 -0.09217153 -0.09114181 -0.00916242 -0.11615587 -0.04525453\n",
      "   0.0888843   0.01958105  0.06044403  0.01635767 -0.09659944 -0.12928379\n",
      "  -0.08904261 -0.04912293 -0.05420295 -0.05420249  0.03163119 -0.08723041\n",
      "   0.01723045  0.04448391 -0.09883722  0.05206111 -0.12219434 -0.02789923\n",
      "   0.00484247  0.08828181 -0.01704518 -0.05449174 -0.0117325  -0.04885237\n",
      "  -0.01065797 -0.07465836 -0.00674751  0.07923209 -0.02573863  0.0394425\n",
      "   0.04149793  0.00735108  0.06526261 -0.03406362 -0.13520269 -0.01710777\n",
      "  -0.06775939 -0.01089635]]\n"
     ]
    }
   ],
   "source": [
    "print(model_w.predict(np.array([8])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW는 Skip gram 방식의 반대입니다. context(문맥)를 넣으면 그것과 연관되는 단어 하나를 output으로 내놓습니다.\n",
    "\n",
    "관심 가져야 하는 부분은 Embedding layer 입니다.\n",
    "\n",
    "lambda layer는 embedding layer를 거쳐 나온 단어 벡터들의 평균치를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "w_size = 10000\n",
    "embed_size = 500\n",
    "window_size = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(w_size, embed_size, embeddings_initializer='glorot_uniform', input_length=window_size * 2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "model.add(Dense(w_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당연히 학습 시 데이터 1:1 균형도 맞추어 주고 학습 시켜야 겠지요?\n",
    "\n",
    "데이터 처리 및 학습과정은 생략하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim 을 이용한 skipgrams 학습 - text8 dataset\n",
    "\n",
    "gensim은 아주 유용한 라이브러리 입니다.\n",
    "\n",
    "주로 topic modeling 과 word embeding 에 많이 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 20:07:57,625 : INFO : collecting all words and their counts\n",
      "2018-01-17 20:07:57,626 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-17 20:07:57,827 : INFO : PROGRESS: at sentence #10000, processed 500000 words, keeping 33464 word types\n",
      "2018-01-17 20:07:58,024 : INFO : PROGRESS: at sentence #20000, processed 1000000 words, keeping 52755 word types\n",
      "2018-01-17 20:07:58,211 : INFO : PROGRESS: at sentence #30000, processed 1500000 words, keeping 65589 word types\n",
      "2018-01-17 20:07:58,399 : INFO : PROGRESS: at sentence #40000, processed 2000000 words, keeping 78383 word types\n",
      "2018-01-17 20:07:58,586 : INFO : PROGRESS: at sentence #50000, processed 2500000 words, keeping 88008 word types\n",
      "2018-01-17 20:07:58,775 : INFO : PROGRESS: at sentence #60000, processed 3000000 words, keeping 96645 word types\n",
      "2018-01-17 20:07:58,958 : INFO : PROGRESS: at sentence #70000, processed 3500000 words, keeping 104309 word types\n",
      "2018-01-17 20:07:59,145 : INFO : PROGRESS: at sentence #80000, processed 4000000 words, keeping 111461 word types\n",
      "2018-01-17 20:07:59,382 : INFO : PROGRESS: at sentence #90000, processed 4500000 words, keeping 118752 word types\n",
      "2018-01-17 20:07:59,614 : INFO : PROGRESS: at sentence #100000, processed 5000000 words, keeping 125355 word types\n",
      "2018-01-17 20:07:59,803 : INFO : PROGRESS: at sentence #110000, processed 5500000 words, keeping 133141 word types\n",
      "2018-01-17 20:08:00,006 : INFO : PROGRESS: at sentence #120000, processed 6000000 words, keeping 139566 word types\n",
      "2018-01-17 20:08:00,330 : INFO : PROGRESS: at sentence #130000, processed 6500000 words, keeping 145782 word types\n",
      "2018-01-17 20:08:00,522 : INFO : PROGRESS: at sentence #140000, processed 7000000 words, keeping 151934 word types\n",
      "2018-01-17 20:08:00,822 : INFO : PROGRESS: at sentence #150000, processed 7500000 words, keeping 158046 word types\n",
      "2018-01-17 20:08:01,030 : INFO : PROGRESS: at sentence #160000, processed 8000000 words, keeping 164115 word types\n",
      "2018-01-17 20:08:01,216 : INFO : PROGRESS: at sentence #170000, processed 8500000 words, keeping 171256 word types\n",
      "2018-01-17 20:08:01,407 : INFO : PROGRESS: at sentence #180000, processed 9000000 words, keeping 178163 word types\n",
      "2018-01-17 20:08:01,591 : INFO : PROGRESS: at sentence #190000, processed 9500000 words, keeping 184129 word types\n",
      "2018-01-17 20:08:01,785 : INFO : PROGRESS: at sentence #200000, processed 10000000 words, keeping 189075 word types\n",
      "2018-01-17 20:08:01,969 : INFO : PROGRESS: at sentence #210000, processed 10500000 words, keeping 194511 word types\n",
      "2018-01-17 20:08:02,153 : INFO : PROGRESS: at sentence #220000, processed 11000000 words, keeping 198758 word types\n",
      "2018-01-17 20:08:02,350 : INFO : PROGRESS: at sentence #230000, processed 11500000 words, keeping 203441 word types\n",
      "2018-01-17 20:08:02,542 : INFO : PROGRESS: at sentence #240000, processed 12000000 words, keeping 207895 word types\n",
      "2018-01-17 20:08:02,736 : INFO : PROGRESS: at sentence #250000, processed 12500000 words, keeping 212668 word types\n",
      "2018-01-17 20:08:02,924 : INFO : PROGRESS: at sentence #260000, processed 13000000 words, keeping 217128 word types\n",
      "2018-01-17 20:08:03,112 : INFO : PROGRESS: at sentence #270000, processed 13500000 words, keeping 221416 word types\n",
      "2018-01-17 20:08:03,304 : INFO : PROGRESS: at sentence #280000, processed 14000000 words, keeping 226855 word types\n",
      "2018-01-17 20:08:03,505 : INFO : PROGRESS: at sentence #290000, processed 14500000 words, keeping 231424 word types\n",
      "2018-01-17 20:08:03,700 : INFO : PROGRESS: at sentence #300000, processed 15000000 words, keeping 237391 word types\n",
      "2018-01-17 20:08:03,892 : INFO : PROGRESS: at sentence #310000, processed 15500000 words, keeping 241697 word types\n",
      "2018-01-17 20:08:04,090 : INFO : PROGRESS: at sentence #320000, processed 16000000 words, keeping 245649 word types\n",
      "2018-01-17 20:08:04,283 : INFO : PROGRESS: at sentence #330000, processed 16500000 words, keeping 249621 word types\n",
      "2018-01-17 20:08:04,470 : INFO : PROGRESS: at sentence #340000, processed 17000000 words, keeping 253834 word types\n",
      "2018-01-17 20:08:04,473 : INFO : collected 253855 word types from a corpus of 17005208 raw words and 340105 sentences\n",
      "2018-01-17 20:08:04,474 : INFO : Loading a fresh vocabulary\n",
      "2018-01-17 20:08:05,743 : INFO : min_count=5 retains 71290 unique words (28% of original 253855, drops 182565)\n",
      "2018-01-17 20:08:05,744 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005208, drops 286364)\n",
      "2018-01-17 20:08:05,967 : INFO : deleting the raw counts dictionary of 253855 items\n",
      "2018-01-17 20:08:05,976 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2018-01-17 20:08:05,977 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2018-01-17 20:08:05,978 : INFO : estimated required memory for 71290 words and 300 dimensions: 206741000 bytes\n",
      "2018-01-17 20:08:06,293 : INFO : resetting layer weights\n",
      "2018-01-17 20:08:07,344 : INFO : training model with 3 workers on 71290 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-01-17 20:08:08,361 : INFO : PROGRESS: at 0.91% examples, 559391 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-17 20:08:09,379 : INFO : PROGRESS: at 1.91% examples, 587937 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:10,385 : INFO : PROGRESS: at 2.92% examples, 603724 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:11,387 : INFO : PROGRESS: at 3.95% examples, 612524 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:12,403 : INFO : PROGRESS: at 4.95% examples, 616019 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:13,407 : INFO : PROGRESS: at 5.95% examples, 618391 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-17 20:08:14,410 : INFO : PROGRESS: at 6.99% examples, 622230 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:15,418 : INFO : PROGRESS: at 8.03% examples, 624312 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:16,427 : INFO : PROGRESS: at 9.03% examples, 625385 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:17,434 : INFO : PROGRESS: at 10.04% examples, 625550 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:18,455 : INFO : PROGRESS: at 11.07% examples, 626155 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:19,465 : INFO : PROGRESS: at 12.09% examples, 627200 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:20,470 : INFO : PROGRESS: at 13.09% examples, 627244 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:21,479 : INFO : PROGRESS: at 14.09% examples, 627477 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:22,495 : INFO : PROGRESS: at 15.14% examples, 627522 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:23,509 : INFO : PROGRESS: at 16.18% examples, 628070 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:24,524 : INFO : PROGRESS: at 17.21% examples, 628309 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:25,529 : INFO : PROGRESS: at 18.24% examples, 628365 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:26,534 : INFO : PROGRESS: at 19.28% examples, 628599 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:27,538 : INFO : PROGRESS: at 20.32% examples, 627855 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:28,543 : INFO : PROGRESS: at 21.39% examples, 628674 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:29,557 : INFO : PROGRESS: at 22.43% examples, 628483 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:30,573 : INFO : PROGRESS: at 23.45% examples, 628349 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:31,584 : INFO : PROGRESS: at 24.49% examples, 628813 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:32,584 : INFO : PROGRESS: at 25.49% examples, 629046 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:33,589 : INFO : PROGRESS: at 26.51% examples, 629666 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-17 20:08:34,605 : INFO : PROGRESS: at 27.52% examples, 629753 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:35,627 : INFO : PROGRESS: at 28.56% examples, 629828 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:36,635 : INFO : PROGRESS: at 29.57% examples, 629951 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:37,645 : INFO : PROGRESS: at 30.59% examples, 630097 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:38,647 : INFO : PROGRESS: at 31.61% examples, 630538 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 20:08:39,648 : INFO : PROGRESS: at 32.63% examples, 630619 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:40,663 : INFO : PROGRESS: at 33.66% examples, 630822 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:41,671 : INFO : PROGRESS: at 34.68% examples, 631133 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:42,694 : INFO : PROGRESS: at 35.75% examples, 630784 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-17 20:08:43,702 : INFO : PROGRESS: at 36.79% examples, 631460 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:44,707 : INFO : PROGRESS: at 37.84% examples, 631863 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:45,722 : INFO : PROGRESS: at 38.86% examples, 631849 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:46,723 : INFO : PROGRESS: at 39.87% examples, 631991 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:47,737 : INFO : PROGRESS: at 40.93% examples, 631960 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:48,740 : INFO : PROGRESS: at 41.96% examples, 632076 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:49,742 : INFO : PROGRESS: at 42.97% examples, 632343 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:50,748 : INFO : PROGRESS: at 44.01% examples, 632357 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-17 20:08:51,759 : INFO : PROGRESS: at 45.03% examples, 632371 words/s, in_qsize 4, out_qsize 1\n",
      "2018-01-17 20:08:52,764 : INFO : PROGRESS: at 46.07% examples, 632764 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:53,766 : INFO : PROGRESS: at 47.09% examples, 633049 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:54,769 : INFO : PROGRESS: at 48.10% examples, 633291 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:55,778 : INFO : PROGRESS: at 49.16% examples, 633459 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:56,778 : INFO : PROGRESS: at 50.16% examples, 633485 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:57,780 : INFO : PROGRESS: at 51.16% examples, 633749 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:58,784 : INFO : PROGRESS: at 52.16% examples, 633548 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:08:59,795 : INFO : PROGRESS: at 53.20% examples, 633637 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:00,798 : INFO : PROGRESS: at 54.21% examples, 633735 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:01,806 : INFO : PROGRESS: at 55.24% examples, 633803 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:02,812 : INFO : PROGRESS: at 56.26% examples, 633705 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:03,827 : INFO : PROGRESS: at 57.30% examples, 633802 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:04,832 : INFO : PROGRESS: at 58.35% examples, 633871 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:05,834 : INFO : PROGRESS: at 59.36% examples, 633803 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:06,837 : INFO : PROGRESS: at 60.46% examples, 633736 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:07,848 : INFO : PROGRESS: at 61.50% examples, 633688 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:08,855 : INFO : PROGRESS: at 62.50% examples, 633716 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:09,856 : INFO : PROGRESS: at 63.50% examples, 633777 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:10,866 : INFO : PROGRESS: at 64.50% examples, 633601 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:11,879 : INFO : PROGRESS: at 65.53% examples, 633782 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:12,891 : INFO : PROGRESS: at 66.54% examples, 633760 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:13,893 : INFO : PROGRESS: at 67.57% examples, 633725 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:14,900 : INFO : PROGRESS: at 68.58% examples, 633709 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:15,906 : INFO : PROGRESS: at 69.60% examples, 633698 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:16,911 : INFO : PROGRESS: at 70.61% examples, 633681 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:17,919 : INFO : PROGRESS: at 71.65% examples, 633634 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:18,929 : INFO : PROGRESS: at 72.66% examples, 633676 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:19,938 : INFO : PROGRESS: at 73.68% examples, 633677 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:20,953 : INFO : PROGRESS: at 74.71% examples, 633609 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:21,960 : INFO : PROGRESS: at 75.78% examples, 633640 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:22,969 : INFO : PROGRESS: at 76.78% examples, 633483 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:23,974 : INFO : PROGRESS: at 77.79% examples, 633471 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:24,977 : INFO : PROGRESS: at 78.82% examples, 633424 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:25,984 : INFO : PROGRESS: at 79.85% examples, 633366 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:26,991 : INFO : PROGRESS: at 80.93% examples, 633329 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:27,998 : INFO : PROGRESS: at 81.94% examples, 633291 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:29,003 : INFO : PROGRESS: at 82.96% examples, 633211 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:30,017 : INFO : PROGRESS: at 83.99% examples, 633088 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:31,019 : INFO : PROGRESS: at 84.97% examples, 632898 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:32,020 : INFO : PROGRESS: at 85.96% examples, 632839 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:33,029 : INFO : PROGRESS: at 86.97% examples, 632788 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:34,034 : INFO : PROGRESS: at 88.00% examples, 632798 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:35,049 : INFO : PROGRESS: at 88.95% examples, 632346 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:36,056 : INFO : PROGRESS: at 89.96% examples, 632249 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:37,072 : INFO : PROGRESS: at 90.91% examples, 631799 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:38,086 : INFO : PROGRESS: at 91.92% examples, 631754 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:39,097 : INFO : PROGRESS: at 92.94% examples, 631681 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:40,102 : INFO : PROGRESS: at 93.94% examples, 631471 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:41,106 : INFO : PROGRESS: at 94.95% examples, 631365 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:42,106 : INFO : PROGRESS: at 95.95% examples, 631088 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:43,113 : INFO : PROGRESS: at 96.95% examples, 630919 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:44,114 : INFO : PROGRESS: at 97.92% examples, 630723 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:45,118 : INFO : PROGRESS: at 98.90% examples, 630510 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:46,123 : INFO : PROGRESS: at 99.85% examples, 630255 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-17 20:09:46,180 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-17 20:09:46,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-17 20:09:46,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-17 20:09:46,189 : INFO : training on 84594700 raw words (62287573 effective words) took 98.8s, 630205 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# text8 데이터를 만들어주는 iterator 입니다.\n",
    "class Text8iter:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        ftext = open('text8', 'r')\n",
    "        self.text = ftext.read().split(\" \")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        words = []\n",
    "        for word in self.text:\n",
    "            words.append(word)\n",
    "            if len(words) == self.size:\n",
    "                yield words\n",
    "                words.clear()\n",
    "        yield words\n",
    "    \n",
    "# 진행상황을 보여주기 위해 logging을 활성화 시킵니다.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# gensim 모듈 호출\n",
    "model = Word2Vec(Text8iter(50), window=10, size=300, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 20:10:50,247 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-01-17 20:10:50,822 : INFO : saving Word2Vec object under word2vec_gensim.bin, separately None\n",
      "2018-01-17 20:10:50,823 : INFO : storing np array 'syn0' to word2vec_gensim.bin.wv.syn0.npy\n",
      "2018-01-17 20:10:50,900 : INFO : not storing attribute syn0norm\n",
      "2018-01-17 20:10:50,901 : INFO : not storing attribute cum_table\n",
      "2018-01-17 20:10:50,901 : INFO : storing np array 'syn1neg' to word2vec_gensim.bin.syn1neg.npy\n",
      "2018-01-17 20:10:51,213 : INFO : saved word2vec_gensim.bin\n",
      "2018-01-17 20:10:51,214 : INFO : loading Word2Vec object from word2vec_gensim.bin\n",
      "2018-01-17 20:10:51,604 : INFO : loading wv recursively from word2vec_gensim.bin.wv.* with mmap=None\n",
      "2018-01-17 20:10:51,606 : INFO : loading syn0 from word2vec_gensim.bin.wv.syn0.npy with mmap=None\n",
      "2018-01-17 20:10:51,648 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-01-17 20:10:51,650 : INFO : loading syn1neg from word2vec_gensim.bin.syn1neg.npy with mmap=None\n",
      "2018-01-17 20:10:51,694 : INFO : setting ignored attribute cum_table to None\n",
      "2018-01-17 20:10:51,695 : INFO : loaded word2vec_gensim.bin\n",
      "2018-01-17 20:10:51,970 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.most_similar(\"woman\")\n",
      "[('felix', 0.4254460334777832), ('forrest', 0.402638703584671), ('cycnus', 0.40224796533584595), ('delegated', 0.4004993438720703), ('ria', 0.3906964659690857), ('lesbian', 0.387756884098053), ('antiope', 0.38243430852890015), ('remus', 0.37411677837371826), ('romulus', 0.37126415967941284), ('silvia', 0.36846351623535156)]\n",
      "\n",
      "model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=10)\n",
      "[('f', 0.3773750364780426), ('kamehameha', 0.3747895359992981), ('goa', 0.3620443344116211), ('iii', 0.36089569330215454), ('disbands', 0.357067346572876), ('lily', 0.35643506050109863), ('administration', 0.34966447949409485), ('dewey', 0.3376150131225586), ('felt', 0.3349570035934448), ('patriarch', 0.33352944254875183)]\n",
      "\n",
      "model.similarity(\"girl\", \"woman\")\n",
      "0.156221421765\n",
      "\n",
      "model.similarity(\"girl\", \"man\")\n",
      "0.0048251359926\n",
      "\n",
      "model.similarity(\"girl\", \"car\")\n",
      "-0.00476152936589\n",
      "\n",
      "model.similarity(\"bus\", \"car\")\n",
      "0.159625484623\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 저장\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"word2vec_gensim.bin\")\n",
    "\n",
    "# 저장된 모델 메모리로 다시 load\n",
    "model = Word2Vec.load(\"word2vec_gensim.bin\")\n",
    "\n",
    "# 여러가지 기능들\n",
    "print(\"\"\"model.most_similar(\"woman\")\"\"\")\n",
    "print(model.wv.most_similar(\"woman\"))\n",
    "print()\n",
    " \n",
    "print(\"\"\"model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=10)\"\"\")\n",
    "print(model.wv.most_similar(positive=['woman', 'king'], \n",
    "                            negative=['man'], \n",
    "                            topn=10))\n",
    "print()       \n",
    "    \n",
    "print(\"\"\"model.similarity(\"girl\", \"woman\")\"\"\")\n",
    "print(model.wv.similarity(\"girl\", \"woman\"))\n",
    "print()\n",
    "print(\"\"\"model.similarity(\"girl\", \"man\")\"\"\")\n",
    "print(model.wv.similarity(\"girl\", \"man\"))\n",
    "print()\n",
    "print(\"\"\"model.similarity(\"girl\", \"car\")\"\"\")\n",
    "print(model.wv.similarity(\"girl\", \"car\"))\n",
    "print()\n",
    "print(\"\"\"model.similarity(\"bus\", \"car\")\"\"\")\n",
    "print(model.wv.similarity(\"bus\", \"car\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과가 잘 나오진 않았습니다 ㅎㅎ.. 파라미터를 조금 잘못 넣은 것 같네요\n",
    "\n",
    "편한 라이브러리임에는 틀림 없습니다! 구글에서 만들었으니 믿음도 가구요~\n",
    "\n",
    "WORD2VEC 이였습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact me\n",
    "케라스를 사랑하는 개발자 입니다.\n",
    "\n",
    "질문, 조언, contribtuion 등 소통은 언제나 환영합니다.\n",
    "\n",
    "Anthony Kim(김동현) : artit.anthony@gmail.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
